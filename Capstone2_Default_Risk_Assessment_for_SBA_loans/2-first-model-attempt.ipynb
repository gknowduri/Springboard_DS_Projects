{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f011fc90",
   "metadata": {},
   "source": [
    "## Initial Modeling Attempt: Small Business Loans with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d1bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffdbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Data/Processed/sba_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b74a8",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24706bd2",
   "metadata": {},
   "source": [
    "To start, we load in the cleaned data from our initial data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afea8014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Name', 'City', 'State', 'Zip', 'Bank', 'BankState', 'NAICS',\n",
       "       'ApprovalDate', 'ApprovalFY', 'Term', 'NoEmp', 'NewExist', 'CreateJob',\n",
       "       'RetainedJob', 'FranchiseCode', 'UrbanRural', 'LowDoc', 'ChgOffDate',\n",
       "       'DisbursementDate', 'DisbursementGross', 'BalanceGross', 'MIS_Status',\n",
       "       'ChgOffPrinGr', 'GrAppv', 'SBA_Appv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Recall the feature names from our data.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5127f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##It appears that I've accidentally added an extra column called `Unnamed: 0`! Let's remove it.\n",
    "df = df.drop(df.columns[0], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45102a81",
   "metadata": {},
   "source": [
    "We choose the features to use in our model. Some features, such as `ChgOffDate` and clearly relate to the eventual fate of the loan, so are not appropriate to use in our model. Similarly, we drop `DisbursementDate`, `DisbursementGross`, `BalanceGross` and `ChgOffPrinGr`. It is not obvious whether CreateJob and RetainedJob refer to projections from the loan application, or later follow-up. We leave them in for now. To simplify the initial model, we drop `ApprovalDate` keep only `ApprovalFY`.\n",
    "\n",
    "We also remove the `ID`, `City`, `Zip`, `Bank`, `BankState` and `FranchiseCode.` These categorical variables have a large number of values, which would create memory issues with one-hot encoding.\n",
    "\n",
    "We subset on the remaining columns, and drop rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "750f20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of features to use for our model\n",
    "features = ['State', 'NAICS', 'Appro valFY', 'Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'UrbanRural','LowDoc', 'MIS_Status', 'GrAppv', 'SBA_Appv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b202f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Appro valFY'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-471d6c24b30c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Subset on relevant columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3028\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3030\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3032\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1314\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Appro valFY'] not in index\""
     ]
    }
   ],
   "source": [
    "## Subset on relevant columns\n",
    "df_pred = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81440834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bc3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = df_pred.RetainedJob.value_counts()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f582caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows with missing values\n",
    "df_pred = df_pred.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd45bef",
   "metadata": {},
   "source": [
    "Since csv format does not keep track of data stypes, we must examine those and reset as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the data types.\n",
    "df_pred.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c305e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns to be converted to categories\n",
    "cat_cols = ['State', 'NAICS', 'NewExist', 'UrbanRural', 'LowDoc', 'MIS_Status']\n",
    "df_pred[cat_cols] = df_pred[cat_cols].apply(lambda x: x.astype('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b907788",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save approval year as an integer\n",
    "df_pred['ApprovalFY'] = df_pred['ApprovalFY'].apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785af78",
   "metadata": {},
   "source": [
    "There are are a large number of NAICS codes, which may create memory issues with one-hot encoding. However, the first two digits of the NAICS code keeps track of the overeall type of industry. We simplify by replacing NAICS code with its first two digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First two digits of each industry code\n",
    "df_pred['Industry'] = df_pred['NAICS'].apply(lambda x: str(x)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred['Industry'] = df_pred['Industry'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa40a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred.drop('NAICS', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53713753",
   "metadata": {},
   "source": [
    "We use dummy variables to encode categorical variables as numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8548e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encode categorical features\n",
    "df_features = pd.get_dummies(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd584140",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect our new list of features\n",
    "df_features.columns\n",
    "len(df_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy variabes encoding creates two columns corresponding to MIS Status.\n",
    "## Drop the MIS_Status_PIF column\n",
    "df_features = df_features.drop(df_features.columns[-25], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41602e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Array of predictors\n",
    "X = df_features.drop('MIS_Status_CHGOFF', axis = 1).values\n",
    "labels = df_features.drop('MIS_Status_CHGOFF', axis = 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column of labels\n",
    "y = df_features['MIS_Status_CHGOFF'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71a89e",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We train a random forrest model, using 20% of our data as a training set. We scale the features using standard scalar.\n",
    "\n",
    "Question: should the scalar be applied to columns representing categorical data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac78992",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data in to test and train\n",
    "## Switch to 70% training, 30 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a Random Forrest model, making sure to scale the data first\n",
    "RF_pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier(random_state = 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model\n",
    "RF_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make predictions\n",
    "y_te_pred = RF_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acurracy score on the test set is nearly 93%. Not bad for a first try!\n",
    "accuracy_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf76b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b06d8b",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "\n",
    "What features are contributing most to the model? What does this tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RF_pipe['randomforestclassifier'].feature_importances_\n",
    "\n",
    "## Create a data frame listing feature names and importances\n",
    "features_ranked = pd.DataFrame(zip(labels, importances), columns = ['feature name', 'importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print a table showing the tope 20 features\n",
    "features_ranked = features_ranked.sort_values('importance', ascending = False)\n",
    "features_ranked.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,8))\n",
    "top_features = features_ranked[:20]\n",
    "sns.barplot(x=top_features['importance'], y=top_features['feature name'], color = 'lightblue')\n",
    "#Add chart labels\n",
    "plt.title('Random Forrest Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5d97c",
   "metadata": {},
   "source": [
    "We note that fiscal year is one of the most important features! This is concerning--how will a model that depends on year-by-year trends be able to predict future data? We need to account for variation between years somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51cfb0",
   "metadata": {},
   "source": [
    "## Correlations among top features\n",
    "\n",
    "Let's see which--if any--of the top features correlate with each other. (Question: Should I remove redundant \"dummy variables\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = list(top_features['feature name'])\n",
    "all_cols.append('MIS_Status_CHGOFF')\n",
    "top_cols = df_features[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d6046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot a heatmap of top twenty features, together with loan status. There are only a few very strong correlations here.\n",
    "plt.figure(figsize = (8, 8))\n",
    "ax = sns.heatmap(top_cols.corr(), xticklabels=True, yticklabels=True, square = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc318f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_year = df_features.groupby('ApprovalFY')['MIS_Status_CHGOFF'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8bf76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot default rates by year. We see that there is a large spike for loans approved in mid-80's, \n",
    "## and another large one for laons approved in the late 2000's.\n",
    "ax = by_year.plot(ylabel = 'Proportion of loans defaulted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_2007 = df_features.loc[df_features['ApprovalFY'] == 2007]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dad5a0",
   "metadata": {},
   "source": [
    "year_2007.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
